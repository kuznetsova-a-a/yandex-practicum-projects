{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9493f301",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "Мы обучим модель классифицировать комментарии на позитивные и негативные. \n",
    "\n",
    "В нашем распоряжении набор данных с разметкой о токсичности правок. Мы будем использовать следующие модели:\n",
    "- дерево решений\n",
    "- случайный лес\n",
    "- логистическая регрессия\n",
    "- стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494af5d",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f9fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b411acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c28205c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91cabc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9c2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69554a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(by='toxic')['toxic'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a7bcc",
   "metadata": {},
   "source": [
    "Мы видим, что в датасете содержится около 16000 комментариев, из которых примерно 10% - токсичные. Мы лемматизируем комментарии с помощью WordNetLemmatizer, дополнительно используя POC-теги (мы использовали метод из статьи https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3064a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8e304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(sentence)\n",
    "    return ' '.join([lemmatizer.lemmatize(w.lower(), get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550b41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc187a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9467bc3a4a4306be731941cf53863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['lemmatized_text'] = data['text'].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926abbd",
   "metadata": {},
   "source": [
    "Разобьем данные выборки на тренировочную, валидационную и тестовую часть в соотношении 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32c3202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data['lemmatized_text']\n",
    "target = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8d2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train, comments_valid_test, target_train, target_valid_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345, stratify=target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06a984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_valid, comments_test, target_valid, target_test = train_test_split(\n",
    "    comments_valid_test, target_valid_test, test_size=0.5, random_state=12345, stratify=target_valid_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad6f874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127433,) (127433,) (15929,) (15929,) (15930,) (15930,)\n"
     ]
    }
   ],
   "source": [
    "print(comments_train.shape, target_train.shape, comments_valid.shape, target_valid.shape, comments_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7468b",
   "metadata": {},
   "source": [
    "Создадим TF-IDF матрицу, которую будем использовать в качестве матрицы признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad72eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a864c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stoplist) \n",
    "tf_idf = count_tf_idf.fit_transform(comments_train) \n",
    "features_train = tf_idf\n",
    "features_valid = count_tf_idf.transform(comments_valid)\n",
    "features_test = count_tf_idf.transform(comments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963d781",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffe867",
   "metadata": {},
   "source": [
    "### Дерево решений\n",
    "Мы видим, что при большой глубине дерева решений значение f1_score достаточно велико (0.68), но всё равно не дотягивает до целевого значения в 0.75, поэтому считаем нерациональным дальнейшие эксперименты с глубиной дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "889a2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = 12345)\n",
    "grid = {'max_depth': (30,40,10)}\n",
    "gdtc = GridSearchCV(dtc, param_grid=grid, scoring='f1', n_jobs=-1)\n",
    "gdtc.fit(features_train, target_train)\n",
    "best_model_dtc = gdtc.best_estimator_\n",
    "predictions_valid = best_model_dtc.predict(features_valid)\n",
    "f1_tree = f1_score(target_valid, predictions_valid)\n",
    "accuracy_tree = accuracy_score(target_valid, predictions_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65c8e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод DecisionTreeClassifier) f1 = 0.6834061135371179\n",
      "На валидационной выборке (метод DecisionTreeClassifier) accuracy = 0.9453826354447863\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод DecisionTreeClassifier) f1 =', f1_tree)\n",
    "print('На валидационной выборке (метод DecisionTreeClassifier) accuracy =', accuracy_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d27963",
   "metadata": {},
   "source": [
    "### Случайный лес\n",
    "Модель даёт низкое значение f1_score и не подходит в данной задаче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27bd8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state = 12345)\n",
    "grid = {'n_estimators': (1,5,1), 'max_depth': (10,20,2)}\n",
    "grfc = GridSearchCV(rfc, param_grid=grid, scoring='f1', n_jobs=-1)\n",
    "grfc.fit(features_train, target_train)\n",
    "best_model_rfc = grfc.best_estimator_\n",
    "predictions_valid = best_model_rfc.predict(features_valid)\n",
    "f1_forest = f1_score(target_valid, predictions_valid)\n",
    "accuracy_forest = accuracy_score(target_valid, predictions_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfe36e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод RandomForestClassifier) f1 = 0.06611570247933883\n",
      "На валидационной выборке (метод RandomForestClassifier) accuracy = 0.9006842865214388\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод RandomForestClassifier) f1 =', f1_forest)\n",
    "print('На валидационной выборке (метод RandomForestClassifier) accuracy =', accuracy_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c3926",
   "metadata": {},
   "source": [
    "### Логистическая регрессия\n",
    "Модель логистической регрессии позволяет достичь целевого значения f1 (f1=0.76), но дополнительно применим метод изменения порога классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0f50482",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 12345, class_weight = 'balanced', solver='liblinear', max_iter = 1000)\n",
    "grid = {'C':(5,15)}\n",
    "glr = GridSearchCV(lr, param_grid=grid, scoring='f1', n_jobs=-1)\n",
    "glr.fit(features_train, target_train)\n",
    "best_model_lr = glr.best_estimator_\n",
    "predictions_valid = best_model_lr.predict(features_valid)\n",
    "f1_regression = f1_score(target_valid, predictions_valid)\n",
    "accuracy_regression = accuracy_score(target_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a41190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод LogisticRegression) f1 = 0.7607223476297967\n",
      "На валидационной выборке (метод LogisticRegression) accuracy = 0.9467637642036537\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод LogisticRegression) f1 =', f1_regression)\n",
    "print('На валидационной выборке (метод LogisticRegression) accuracy =', accuracy_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f3fe8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test = best_model_lr.predict_proba(features_test)\n",
    "probabilities_valid = best_model_lr.predict_proba(features_valid)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold_lr = 0\n",
    "for threshold in np.arange(0, 1, 0.01):\n",
    "    predictions_valid  = probabilities_valid[:,1] > threshold\n",
    "    if f1_score(target_valid, predictions_valid) > best_f1:\n",
    "        best_threshold_lr = threshold\n",
    "        best_f1 = f1_score(target_valid, predictions_valid)\n",
    "        \n",
    "predictions_valid  = probabilities_valid[:,1] > best_threshold_lr\n",
    "   \n",
    "f1_regression_threshold = f1_score(target_valid, predictions_valid)\n",
    "accuracy_regression_threshold = accuracy_score(target_valid, predictions_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ead94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод LogisticRegression + подбор порога классификации) f1 = 0.7902408111533588\n",
      "На валидационной выборке (метод LogisticRegression + подбор порога классификации) accuracy = 0.9584405800740787\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод LogisticRegression + подбор порога классификации) f1 =', f1_regression_threshold)\n",
    "print('На валидационной выборке (метод LogisticRegression + подбор порога классификации) accuracy =', accuracy_regression_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca281e",
   "metadata": {},
   "source": [
    "### Стохастический градиентный спуск\n",
    "Как и логистическая регрессия, стохастический градиентный спуск дает значение f1 близкое к целевому, а после дополнительного подбора порога классификации f1=0.78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12910041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc = SGDClassifier(random_state = 12345, loss='modified_huber', penalty=\"l2\", class_weight = 'balanced', max_iter = 1000)\n",
    "grid = {}\n",
    "gsgdc = GridSearchCV(sgdc, param_grid=grid, scoring='f1', n_jobs=-1)\n",
    "gsgdc.fit(features_train, target_train)\n",
    "best_model_sgdc = gsgdc.best_estimator_\n",
    "predictions_valid = best_model_sgdc.predict(features_valid)\n",
    "f1_SGDC =  f1_score(target_valid, predictions_valid)\n",
    "accuracy_SGDC =  accuracy_score(target_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aef01d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод SGDClassifier) f1 = 0.7463984778472411\n",
      "На валидационной выборке (метод SGDClassifier) accuracy = 0.9414275849080294\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод SGDClassifier) f1 =', f1_SGDC)\n",
    "print('На валидационной выборке (метод SGDClassifier) accuracy =', accuracy_SGDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aca1256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_valid = best_model_sgdc.predict_proba(features_valid)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold_SGD = 0\n",
    "for threshold in np.arange(0, 1, 0.01):\n",
    "    predictions_valid  = probabilities_valid[:,1] > threshold\n",
    "    if f1_score(target_valid, predictions_valid) > best_f1:\n",
    "        best_threshold_SGD = threshold\n",
    "        best_f1 = f1_score(target_valid, predictions_valid)\n",
    "        \n",
    "predictions_valid  = probabilities_valid[:,1] > best_threshold_SGD\n",
    "   \n",
    "f1_SGDC_threshold = f1_score(target_valid, predictions_valid)\n",
    "accuracy_SGDC_threshold = accuracy_score(target_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "981568f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На валидационной выборке (метод SGDClassifier + подбор порога классификации) f1 = 0.7932354818123802\n",
      "На валидационной выборке (метод SGDClassifier + подбор порога классификации) accuracy = 0.959319480193358\n"
     ]
    }
   ],
   "source": [
    "print('На валидационной выборке (метод SGDClassifier + подбор порога классификации) f1 =', f1_SGDC_threshold)\n",
    "print('На валидационной выборке (метод SGDClassifier + подбор порога классификации) accuracy =', accuracy_SGDC_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54bba1",
   "metadata": {},
   "source": [
    "### Проверка моделей на адекватность, сравнение с константной моделью\n",
    "Составим вектор предсказаний, состоящий из нулей (нет токсичных комментариев). Мы видим, что значение accuracy составляет около 0.9 (10% токсичных комментариев, как мы и видели в датасете), при этом accuracy ниже, чем для моделей логистической регрессии и стохастического спуска, таким образом, построенные выше модели адекватны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8044f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_const = [0]*len(target_valid)\n",
    "accuracy_const = accuracy_score(target_valid, predictions_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d696dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8984242576432921\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea54db",
   "metadata": {},
   "source": [
    "## Вывод и итоговое тестирование\n",
    "Лучшими моделями оказались модели **стохастического градиентного спуска** и **логистической регрессии**, но обе дополнительно потребовали подбора порога классификации. Проведем итоговое тестирование на двух лучших моделях и составим таблицу. Видим, что обе модели проходят итоговое тестирование со значением f1=0.78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67e7b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test_lr = best_model_lr.predict_proba(features_test)\n",
    "predictions_test_lr  = probabilities_test_lr[:,1] > best_threshold_lr\n",
    "f1_lr_test =  f1_score(target_test, predictions_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7938f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_test_SGD = best_model_sgdc.predict_proba(features_test)\n",
    "predictions_test_SGD  = probabilities_test_SGDC[:,1] > best_threshold_SGD\n",
    "f1_SGDC_test =  f1_score(target_test, predictions_test_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6559c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(data = [['DecisionTreeClassifier', f1_tree, accuracy_tree, '-'],\n",
    "                              ['RandomForestClassifier', f1_forest, accuracy_forest, '-'],\n",
    "                              ['LogisticRegression + threshold', f1_regression_threshold, accuracy_regression_threshold, f1_lr_test],\n",
    "                              ['SGDClassifier + threshold', f1_SGDC_threshold, accuracy_SGDC_threshold, f1_SGDC_test],\n",
    "                              ['Constant', '-', accuracy_const, '-']],                     \n",
    "                      columns = ['Model', 'f1_score_valid', 'accuracy_score_valid', 'f1_score_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3584e5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>f1_score_valid</th>\n",
       "      <th>accuracy_score_valid</th>\n",
       "      <th>f1_score_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.683406</td>\n",
       "      <td>0.945383</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.900684</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression + threshold</td>\n",
       "      <td>0.790241</td>\n",
       "      <td>0.958441</td>\n",
       "      <td>0.782636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGDClassifier + threshold</td>\n",
       "      <td>0.793235</td>\n",
       "      <td>0.959319</td>\n",
       "      <td>0.780365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Constant</td>\n",
       "      <td>-</td>\n",
       "      <td>0.898424</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model f1_score_valid  accuracy_score_valid  \\\n",
       "0          DecisionTreeClassifier       0.683406              0.945383   \n",
       "1          RandomForestClassifier       0.066116              0.900684   \n",
       "2  LogisticRegression + threshold       0.790241              0.958441   \n",
       "3       SGDClassifier + threshold       0.793235              0.959319   \n",
       "4                        Constant              -              0.898424   \n",
       "\n",
       "  f1_score_test  \n",
       "0             -  \n",
       "1             -  \n",
       "2      0.782636  \n",
       "3      0.780365  \n",
       "4             -  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2015,
    "start_time": "2022-11-16T16:17:17.346Z"
   },
   {
    "duration": 2849,
    "start_time": "2022-11-16T16:17:24.530Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-16T16:17:29.708Z"
   },
   {
    "duration": 35,
    "start_time": "2022-11-16T16:17:32.182Z"
   },
   {
    "duration": 16,
    "start_time": "2022-11-16T16:17:34.286Z"
   },
   {
    "duration": 10,
    "start_time": "2022-11-16T16:17:36.610Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T16:17:40.042Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T16:17:42.205Z"
   },
   {
    "duration": 54,
    "start_time": "2022-11-16T16:18:40.260Z"
   },
   {
    "duration": 137,
    "start_time": "2022-11-16T16:18:57.140Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T16:19:18.619Z"
   },
   {
    "duration": 1589,
    "start_time": "2022-11-16T16:33:31.586Z"
   },
   {
    "duration": 1009,
    "start_time": "2022-11-16T16:33:36.142Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-16T16:33:38.735Z"
   },
   {
    "duration": 35,
    "start_time": "2022-11-16T16:33:41.040Z"
   },
   {
    "duration": 14,
    "start_time": "2022-11-16T16:33:43.007Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-16T16:33:44.859Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T16:33:47.101Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T16:33:48.879Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T16:33:50.944Z"
   },
   {
    "duration": 1654245,
    "start_time": "2022-11-16T16:33:53.121Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:01:42.710Z"
   },
   {
    "duration": 88,
    "start_time": "2022-11-16T17:01:44.446Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-16T17:01:45.319Z"
   },
   {
    "duration": 6,
    "start_time": "2022-11-16T17:01:53.454Z"
   },
   {
    "duration": 8206,
    "start_time": "2022-11-16T17:01:55.781Z"
   },
   {
    "duration": 462392,
    "start_time": "2022-11-16T17:02:08.329Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-16T17:09:50.723Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:10:05.102Z"
   },
   {
    "duration": 0,
    "start_time": "2022-11-16T17:19:50.685Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T17:20:00.725Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-16T17:20:02.333Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-16T17:20:13.268Z"
   },
   {
    "duration": 325951,
    "start_time": "2022-11-16T17:20:20.806Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T17:27:26.407Z"
   },
   {
    "duration": 31270,
    "start_time": "2022-11-16T17:27:48.699Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-16T17:28:49.263Z"
   },
   {
    "duration": 255861,
    "start_time": "2022-11-16T17:28:53.405Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:34:23.489Z"
   },
   {
    "duration": 16,
    "start_time": "2022-11-16T17:38:08.584Z"
   },
   {
    "duration": 801,
    "start_time": "2022-11-16T17:38:24.527Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T17:38:27.976Z"
   },
   {
    "duration": 4498,
    "start_time": "2022-11-16T17:38:39.507Z"
   },
   {
    "duration": 19,
    "start_time": "2022-11-16T17:39:02.431Z"
   },
   {
    "duration": 3522,
    "start_time": "2022-11-16T17:39:11.098Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:39:17.303Z"
   },
   {
    "duration": 65,
    "start_time": "2022-11-16T17:39:25.892Z"
   },
   {
    "duration": 4547,
    "start_time": "2022-11-16T17:39:33.524Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T17:40:11.980Z"
   },
   {
    "duration": 873,
    "start_time": "2022-11-16T17:40:14.739Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:40:26.033Z"
   },
   {
    "duration": 7,
    "start_time": "2022-11-16T17:40:32.404Z"
   },
   {
    "duration": 3,
    "start_time": "2022-11-16T17:40:35.118Z"
   },
   {
    "duration": 13,
    "start_time": "2022-11-16T17:40:49.163Z"
   },
   {
    "duration": 12,
    "start_time": "2022-11-16T17:40:50.303Z"
   },
   {
    "duration": 11,
    "start_time": "2022-11-16T17:41:31.783Z"
   },
   {
    "duration": 54,
    "start_time": "2022-11-16T17:41:33.942Z"
   },
   {
    "duration": 49,
    "start_time": "2022-11-16T17:42:04.473Z"
   },
   {
    "duration": 41,
    "start_time": "2022-11-16T17:42:16.302Z"
   },
   {
    "duration": 15,
    "start_time": "2022-11-16T17:42:39.955Z"
   },
   {
    "duration": 4,
    "start_time": "2022-11-16T17:43:03.219Z"
   },
   {
    "duration": 9,
    "start_time": "2022-11-16T17:43:06.009Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
